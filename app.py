import sys
import os
from functools import lru_cache
from dotenv import load_dotenv

from langchain_core.messages import HumanMessage, AIMessage
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.document_loaders import PyPDFLoader
from langchain_community.vectorstores import FAISS
from langchain_text_splitters import RecursiveCharacterTextSplitter

# .env íŒŒì¼ì—ì„œ API í‚¤ ë¡œë“œ
load_dotenv()
if not os.getenv("OPENAI_API_KEY"):
    print("âŒ OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")
    sys.exit(1)


@lru_cache(maxsize=1)
def get_llm():
    """LLM ì´ˆê¸°í™” - ì•± ì‹¤í–‰ ì‹œ í•œ ë²ˆë§Œ ìƒì„±"""
    print("ğŸ¤– LLMì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤... (OpenAI: gpt-4o-mini)")
    return ChatOpenAI(
        model="gpt-4o-mini",
        max_tokens=2000,
        temperature=0.7,
        streaming=True,
    )


@lru_cache(maxsize=1)
def get_retriever_from_pdf(pdf_path: str):
    """PDF íŒŒì¼ë¡œë¶€í„° Knowledge Base Retriever ì´ˆê¸°í™”"""
    if not os.path.exists(pdf_path):
        print(f"âŒ PDF íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {pdf_path}")
        return None
    
    try:
        print(f"ğŸ“š PDF ë¬¸ì„œë¥¼ ë¡œë“œí•©ë‹ˆë‹¤: {pdf_path}")
        # 1. ë¬¸ì„œ ë¡œë“œ
        loader = PyPDFLoader(pdf_path)
        docs = loader.load()

        # 2. ë¬¸ì„œ ë¶„í• 
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
        splits = text_splitter.split_documents(docs)

        # 3. ì„ë² ë”© ë° ë²¡í„° ìŠ¤í† ì–´ ìƒì„±
        print("ğŸ§  ë¬¸ì„œë¥¼ ì„ë² ë”©í•˜ê³  ë²¡í„° ìŠ¤í† ì–´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤... (FAISS)")
        embeddings = OpenAIEmbeddings()
        vector_store = FAISS.from_documents(splits, embeddings)

        # 4. Retriever ë°˜í™˜
        return vector_store.as_retriever(
            search_kwargs={"k": 5} # ì›ë³¸ ì½”ë“œì˜ numberOfResults=5ì™€ ë™ì¼
        )
    except Exception as e:
        print(f"âŒ Retriever ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return None


def format_docs(docs):
    """ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜ (LangChain Document ê°ì²´ìš©)"""
    if not docs:
        print("âš ï¸ ê²€ìƒ‰ëœ ë¬¸ì„œ ì—†ìŒ")
        return "ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    # LangChainì˜ Document ê°ì²´ì—ì„œ page_contentë¥¼ ë°”ë¡œ ì¶”ì¶œ
    formatted = [
        doc.page_content for doc in docs if hasattr(doc, "page_content") and doc.page_content
    ]

    result = (
        "\n\n---\n\n".join(formatted)
        if formatted
        else "ë¬¸ì„œ ë‚´ìš©ì„ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    )
    print(f"âœ… {len(formatted)}ê°œ ë¬¸ì„œ í¬ë§· ì™„ë£Œ (ì´ {len(result)}ì)")
    return result


def create_chain_with_kb(retriever, llm):
    """RAG ì²´ì¸ ìƒì„± - Retrieverë¡œ ë¬¸ì„œ ê²€ìƒ‰ í›„ LLMì— ì „ë‹¬"""
    prompt = ChatPromptTemplate.from_messages(
        [
            (
                "system",
                """ë‹¤ìŒ ë¬¸ë§¥(context)ì„ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.
ë¬¸ë§¥ì— ë‹µì´ ì—†ìœ¼ë©´ ëª¨ë¥¸ë‹¤ê³  ë‹µí•˜ì„¸ìš”.

Context:
{context}
""",
            ),
            MessagesPlaceholder(variable_name="chat_history"),
            ("human", "{input}"),
        ]
    )

    def retrieve_and_format(x):
        """ê²€ìƒ‰ ì‹¤í–‰ ë° í¬ë§·íŒ…"""
        try:
            input_text = x["input"] if isinstance(x, dict) else x
            print(f"\nğŸ” ê²€ìƒ‰ ì¿¼ë¦¬: '{input_text}'")

            retrieved_docs = retriever.invoke(input_text)
            print(f"ğŸ“Š ê²€ìƒ‰ ê²°ê³¼: {len(retrieved_docs) if retrieved_docs else 0}ê°œ")

            return format_docs(retrieved_docs)
        except Exception as e:
            print(f"âŒ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return f"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"

    # ì²´ì¸ êµ¬ì„±: ê²€ìƒ‰ â†’ í”„ë¡¬í”„íŠ¸ â†’ LLM
    return (
        {
            "context": retrieve_and_format,
            "chat_history": lambda x: x["chat_history"],
            "input": lambda x: x["input"],
        }
        | prompt
        | llm
    )


def create_chain_without_kb(llm):
    """ì¼ë°˜ ëŒ€í™”ìš© ì²´ì¸ - KB ì—†ì´ LLMë§Œ ì‚¬ìš©"""
    prompt = ChatPromptTemplate.from_messages(
        [
            MessagesPlaceholder(variable_name="chat_history"),
            ("human", "{input}"),
        ]
    )
    return prompt | llm


def main():
    """ë©”ì¸ ì±—ë´‡ ì‹¤í–‰ ë¡œì§"""
    llm = get_llm()
    retriever = None
    
    # RAG ì‚¬ìš© ì—¬ë¶€ ë° PDF íŒŒì¼ ê²½ë¡œ ì…ë ¥
    use_kb_input = input("ğŸ’¡ Knowledge Base (RAG)ë¥¼ ì‚¬ìš©í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ").strip().lower()
    
    if use_kb_input == 'y':
        pdf_path = input("ğŸ“‚ ì‚¬ìš©í•  PDF íŒŒì¼ ê²½ë¡œë¥¼ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: document.pdf): ").strip()
        retriever = get_retriever_from_pdf(pdf_path)
        if retriever:
            print("âœ… Knowledge Baseê°€ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤. RAG ëª¨ë“œë¡œ ì‹œì‘í•©ë‹ˆë‹¤.")
        else:
            print("âš ï¸ Knowledge Base ì¤€ë¹„ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ì¼ë°˜ ëŒ€í™” ëª¨ë“œë¡œ ì‹œì‘í•©ë‹ˆë‹¤.")
    else:
        print("â„¹ï¸ ì¼ë°˜ ëŒ€í™” ëª¨ë“œë¡œ ì‹œì‘í•©ë‹ˆë‹¤.")

    # LangChain í˜•ì‹ì˜ ëŒ€í™” ê¸°ë¡
    chat_history = []

    print("\n--- ì±—ë´‡ ì‹œì‘ --- (ì¢…ë£Œí•˜ë ¤ë©´ 'exit' ë˜ëŠ” 'quit' ì…ë ¥)")

    while True:
        try:
            user_input = input("\nYou: ")
            if user_input.lower() in ["exit", "quit"]:
                print("ğŸ‘‹ ì±—ë´‡ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break

            # ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ê¸°ë¡ì— ì¶”ê°€
            chat_history.append(HumanMessage(content=user_input))

            # KB ì‚¬ìš© ì—¬ë¶€ì— ë”°ë¼ ì²´ì¸ ì„ íƒ
            if retriever:
                chain = create_chain_with_kb(retriever, llm)
            else:
                chain = create_chain_without_kb(llm)

            print("AI: ", end="", flush=True)
            
            full_response = ""
            
            # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì²˜ë¦¬
            # chain.stream í˜¸ì¶œ ì‹œ, í˜„ì¬ ì…ë ¥ì„ ì œì™¸í•œ ì´ì „ ê¸°ë¡ì„ ì „ë‹¬
            for chunk in chain.stream(
                {
                    "chat_history": chat_history[:-1], # ë§ˆì§€ë§‰ HumanMessage ì œì™¸
                    "input": user_input,
                }
            ):
                content = chunk.content
                print(content, end="", flush=True)
                full_response += content

            print() # ì¤„ë°”ê¿ˆ

            # AI ì‘ë‹µì„ ê¸°ë¡ì— ì¶”ê°€
            chat_history.append(AIMessage(content=full_response))

        except KeyboardInterrupt:
            print("\nğŸ‘‹ ì±—ë´‡ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break
        except Exception as e:
            error_msg = f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
            print(f"\nâŒ {error_msg}")
            # ì˜¤ë¥˜ ë°œìƒ ì‹œ, AI ì‘ë‹µìœ¼ë¡œ ì˜¤ë¥˜ ë©”ì‹œì§€ ê¸°ë¡
            chat_history.append(AIMessage(content=error_msg))


if __name__ == "__main__":
    main()
