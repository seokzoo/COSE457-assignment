import sys
import os
import gradio as gr
from functools import lru_cache
from dotenv import load_dotenv

from langchain_core.messages import HumanMessage, AIMessage
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.document_loaders import PyPDFLoader
from langchain_community.vectorstores import FAISS
from langchain_text_splitters import RecursiveCharacterTextSplitter

# .env íŒŒì¼ì—ì„œ API í‚¤ ë¡œë“œ
load_dotenv()
if not os.getenv("OPENAI_API_KEY"):
    print("âŒ OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")
    sys.exit(1)


@lru_cache(maxsize=1)
def get_llm():
    """LLM ì´ˆê¸°í™” - ì•± ì‹¤í–‰ ì‹œ í•œ ë²ˆë§Œ ìƒì„±"""
    print("ğŸ¤– LLMì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤... (OpenAI: gpt-4o-mini)")
    return ChatOpenAI(
        model="gpt-4o-mini",
        max_tokens=2000,
        temperature=0.7,
        streaming=True,
    )

def get_retriever_from_pdf(pdf_paths_str: str):
    """ì‰¼í‘œë¡œ êµ¬ë¶„ëœ PDF íŒŒì¼ ê²½ë¡œë“¤ë¡œë¶€í„° Knowledge Base Retriever ì´ˆê¸°í™”"""
    # ì‰¼í‘œë¡œ êµ¬ë¶„ëœ ë¬¸ìì—´ì„ íŒŒì¼ ê²½ë¡œ ëª©ë¡ìœ¼ë¡œ ë³€í™˜í•˜ê³ , ê³µë°± ì œê±°
    pdf_paths = [p.strip() for p in pdf_paths_str.split(',') if p.strip()]
    
    if not pdf_paths:
        raise gr.Error("Knowledge Baseì— ì‚¬ìš©í•  PDF íŒŒì¼ ê²½ë¡œë¥¼ í•˜ë‚˜ ì´ìƒ ì…ë ¥í•´ì•¼ í•©ë‹ˆë‹¤.")

    all_docs = []
    
    # 1. ë¬¸ì„œ ë¡œë“œ (ì—¬ëŸ¬ íŒŒì¼ ì²˜ë¦¬)
    for pdf_path in pdf_paths:
        if not os.path.exists(pdf_path):
            print(f"âŒ PDF íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {pdf_path}")
            continue 
            
        try:
            print(f"ğŸ“š PDF ë¬¸ì„œë¥¼ ë¡œë“œí•©ë‹ˆë‹¤: {pdf_path}")
            loader = PyPDFLoader(pdf_path)
            docs = loader.load()
            if docs:
                all_docs.extend(docs)
            else:
                print(f"âš ï¸ {pdf_path}ì—ì„œ ë¡œë“œëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
        except Exception as e:
            print(f"âŒ {pdf_path} ë¡œë“œ ì‹¤íŒ¨: {e}")
            continue

    if not all_docs:
        raise gr.Error("ìœ íš¨í•œ PDF íŒŒì¼ì—ì„œ ë¬¸ì„œë¥¼ ë¡œë“œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. íŒŒì¼ ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”.")

    try:
        # 2. ë¬¸ì„œ ë¶„í•  (í†µí•©ëœ ë¬¸ì„œ ëª©ë¡ ì‚¬ìš©)
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
        splits = text_splitter.split_documents(all_docs)

        # 3. ì„ë² ë”© ë° ë²¡í„° ìŠ¤í† ì–´ ìƒì„± (í•˜ë‚˜ì˜ í†µí•©ëœ Vector Store)
        print(f"ğŸ§  {len(splits)}ê°œ ë¶„í• ëœ ë¬¸ì„œë¥¼ ì„ë² ë”©í•˜ê³  ë²¡í„° ìŠ¤í† ì–´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤... (FAISS)")
        embeddings = OpenAIEmbeddings()
        vector_store = FAISS.from_documents(splits, embeddings)

        # 4. Retriever ë°˜í™˜
        return vector_store.as_retriever(
            search_kwargs={"k": 5}
        )
    except Exception as e:
        print(f"âŒ Retriever ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        raise gr.Error(f"Retriever ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")

def format_docs(docs):
    """ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜ (LangChain Document ê°ì²´ìš©)"""
    if not docs:
        print("âš ï¸ ê²€ìƒ‰ëœ ë¬¸ì„œ ì—†ìŒ")
        return "ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    formatted_parts = []

    for i, doc in enumerate(docs):
        content = doc.page_content if hasattr(doc, "page_content") and doc.page_content else "ë‚´ìš© ì—†ìŒ"
        
        source_path = doc.metadata.get("source", "ì¶œì²˜ ì •ë³´ ì—†ìŒ")
        source_name = os.path.basename(source_path)
        
        formatted_part = (
            f"--- [document {i+1}] íŒŒì¼ëª…: {source_name} ---\n"
            f"{content}"
        )
        formatted_parts.append(formatted_part)

    result = "\n\n---\n\n".join(formatted_parts)
    print(f"{len(formatted_parts)}ê°œ ë¬¸ì„œ í¬ë§· ì™„ë£Œ (ì´ {len(result)}ì)")
    return result

def create_chain_with_kb(retriever, llm):
    """RAG ì²´ì¸ ìƒì„± - Retrieverë¡œ ë¬¸ì„œ ê²€ìƒ‰ í›„ LLMì— ì „ë‹¬"""
    prompt = ChatPromptTemplate.from_messages(
        [
            (
                "system",
                """ë‹¹ì‹ ì€ ì‚¬ìš©ì ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” ì¹œì ˆí•œ ì±—ë´‡ì…ë‹ˆë‹¤.
ë‹¤ìŒ ë¬¸ë§¥(context)ì„ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.
**ë‹µë³€ ì‹œ, ì°¸ì¡°í•œ ë‚´ìš©ì˜ íŒŒì¼ëª…ì„ ë°˜ë“œì‹œ ê´„í˜¸ ì•ˆì— ëª…ì‹œí•˜ì„¸ìš”.** (ì˜ˆ: ...ì…ë‹ˆë‹¤. (file1.pdf))
ë¬¸ë§¥ì— ë‹µì´ ì—†ìœ¼ë©´ ëª¨ë¥¸ë‹¤ê³  ë‹µí•˜ì„¸ìš”.

Context:
{context}
""",
            ),
            MessagesPlaceholder(variable_name="chat_history"),
            ("human", "{input}"),
        ]
    )

    def retrieve_and_format(x):
        """ê²€ìƒ‰ ì‹¤í–‰ ë° í¬ë§·íŒ…"""
        try:
            input_text = x["input"] if isinstance(x, dict) else x
            print(f"\nğŸ” ê²€ìƒ‰ ì¿¼ë¦¬: '{input_text}'")
            retrieved_docs = retriever.invoke(input_text)
            print(f"ğŸ“Š ê²€ìƒ‰ ê²°ê³¼: {len(retrieved_docs) if retrieved_docs else 0}ê°œ")
            return format_docs(retrieved_docs)
        except Exception as e:
            print(f"âŒ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return f"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"

    return (
        {
            "context": retrieve_and_format,
            "chat_history": lambda x: x["chat_history"],
            "input": lambda x: x["input"],
        }
        | prompt
        | llm
    )


def create_chain_without_kb(llm):
    """ì¼ë°˜ ëŒ€í™”ìš© ì²´ì¸ - KB ì—†ì´ LLMë§Œ ì‚¬ìš©"""
    prompt = ChatPromptTemplate.from_messages(
        [
            MessagesPlaceholder(variable_name="chat_history"),
            ("human", "{input}"),
        ]
    )
    return prompt | llm


# --- Gradio UI ë° ì±„íŒ… ë¡œì§ ---

llm = get_llm() # LLMì€ ì•± ì‹œì‘ ì‹œ í•œ ë²ˆë§Œ ë¡œë“œ

def chat_response(user_input, chat_history_list, use_kb, pdf_path):
    """
    Gradioì˜ Chatbot UIì™€ ì—°ë™ë˜ëŠ” ë©”ì¸ í•¨ìˆ˜.
    user_input: ì‚¬ìš©ìì˜ ìƒˆ ì…ë ¥ (str)
    chat_history_list: Gradio ì±—ë´‡ì˜ ëŒ€í™” ê¸°ë¡ (List[List[str, str]])
    use_kb: KB ì‚¬ìš© ì—¬ë¶€ (bool)
    pdf_path: PDF íŒŒì¼ ê²½ë¡œ (str)
    """
    
    # 1. Gradioì˜ ëŒ€í™” ê¸°ë¡ì„ LangChain í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    langchain_history = []
    for user_msg, ai_msg in chat_history_list:
        if user_msg:
            langchain_history.append(HumanMessage(content=user_msg))
        if ai_msg:
            langchain_history.append(AIMessage(content=ai_msg))
    
    # 2. ì²´ì¸ ì„ íƒ
    try:
        if use_kb:
            if not pdf_path:
                raise gr.Error("Knowledge Baseë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ PDF íŒŒì¼ ê²½ë¡œë¥¼ ì…ë ¥í•´ì•¼ í•©ë‹ˆë‹¤.")
            # ìºì‹œëœ Retriever ê°€ì ¸ì˜¤ê¸°
            retriever = get_retriever_from_pdf(pdf_path)
            chain = create_chain_with_kb(retriever, llm)
            print("â„¹ï¸ RAG ëª¨ë“œë¡œ ì‘ë‹µí•©ë‹ˆë‹¤.")
        else:
            chain = create_chain_without_kb(llm)
            print("â„¹ï¸ ì¼ë°˜ ëŒ€í™” ëª¨ë“œë¡œ ì‘ë‹µí•©ë‹ˆë‹¤.")
            
    except Exception as e:
        # get_retriever_from_pdfì—ì„œ ì˜¤ë¥˜ ë°œìƒ ì‹œ
        yield chat_history_list + [[user_input, str(e)]]
        return

    # 3. ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì²˜ë¦¬
    full_response = ""
    # UIì— ì‚¬ìš©ì ë©”ì‹œì§€ ë¨¼ì € í‘œì‹œ
    chat_history_list.append([user_input, ""])
    
    try:
        # streaming
        for chunk in chain.stream(
            {
                "chat_history": langchain_history, # í˜„ì¬ ì…ë ¥ì„ ì œì™¸í•œ ì´ì „ ê¸°ë¡
                "input": user_input,
            }
        ):
            full_response += chunk.content
            # ë§ˆì§€ë§‰ ë©”ì‹œì§€(AI ì‘ë‹µ)ë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ ì—…ë°ì´íŠ¸
            chat_history_list[-1][1] = full_response
            yield chat_history_list
            
    except Exception as e:
        error_msg = f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
        print(f"âŒ {error_msg}")
        chat_history_list[-1][1] = error_msg
        yield chat_history_list


def clear_chat():
    """ì±„íŒ… ê¸°ë¡ì„ ì§€ìš°ëŠ” í•¨ìˆ˜"""
    return [], []

def update_status_text(use_kb, pdf_paths_str):
    """KB ìƒíƒœ í‘œì‹œì¤„ ì—…ë°ì´íŠ¸ (ì—¬ëŸ¬ íŒŒì¼ ì§€ì›)"""
    if use_kb:
        pdf_paths = [p.strip() for p in pdf_paths_str.split(',') if p.strip()]
        
        if not pdf_paths:
             return "âš ï¸ KB ì‚¬ìš© ì²´í¬ë¨ (PDF ê²½ë¡œ ì—†ìŒ)"
        
        valid_files = [p for p in pdf_paths if os.path.exists(p)]
        invalid_files = [p for p in pdf_paths if not os.path.exists(p)]
        
        status = f"âœ… KB ì‚¬ìš© ì¤‘ ({len(valid_files)}ê°œ íŒŒì¼)"
        if invalid_files:
             status += f" âš ï¸ ({len(invalid_files)}ê°œ íŒŒì¼ ê²½ë¡œ ì˜¤ë¥˜)"
             
        return status
    else:
        return "â„¹ï¸ ì¼ë°˜ ëŒ€í™” ëª¨ë“œ"

# --- Gradio UI ë ˆì´ì•„ì›ƒ ---
pdfs = "jeff_bezos_1997.pdf,warren_buffett_2025.pdf"
with gr.Blocks(theme=gr.themes.Soft()) as demo:
    gr.Markdown("# ğŸ¤– Chatbot with PDF Knowledge Base (Gradio)")

    with gr.Row():
        # --- 1. ì‚¬ì´ë“œë°” (Streamlitì˜ sidebarì™€ ìœ ì‚¬) ---
        with gr.Column(scale=1, min_width=300):
            gr.Markdown("## âš™ï¸ ì„¤ì •")
            
            pdf_path_input = gr.Textbox(
                label="PDF íŒŒì¼ ê²½ë¡œ (ì‰¼í‘œ êµ¬ë¶„)",
                value=pdfs,
                info="RAGì— ì‚¬ìš©í•  PDF íŒŒì¼ì˜ ê²½ë¡œë“¤ì„ ì‰¼í‘œ(,)ë¡œ êµ¬ë¶„í•˜ì—¬ ì…ë ¥í•˜ì„¸ìš”.",
            )
            use_kb_checkbox = gr.Checkbox(
                label="Knowledge Base ì‚¬ìš©", 
                value=True
            )
            status_display = gr.Markdown(
                update_status_text(True, pdfs) # ì´ˆê¸° ìƒíƒœ
            )
            
            # ì„¤ì • ë³€ê²½ ì‹œ ìƒíƒœ í…ìŠ¤íŠ¸ ì—…ë°ì´íŠ¸
            use_kb_checkbox.change(fn=update_status_text, inputs=[use_kb_checkbox, pdf_path_input], outputs=status_display)
            pdf_path_input.change(fn=update_status_text, inputs=[use_kb_checkbox, pdf_path_input], outputs=status_display)

        # --- 2. ë©”ì¸ ì±—ë´‡ ì°½ ---
        with gr.Column(scale=4):
            chatbot_display = gr.Chatbot(
                label="Chat",
                height=600,
                bubble_full_width=False
            )
            user_input_textbox = gr.Textbox(
                placeholder="Message OpenAI...",
                label="User Input",
                scale=7
            )
            clear_btn = gr.Button("ğŸ—‘ï¸ Clear Chat")

    # --- 3. ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬ ì—°ê²° ---
    
    # ì‚¬ìš©ì ì…ë ¥(submit) ì‹œ
    user_input_textbox.submit(
        fn=chat_response,
        inputs=[user_input_textbox, chatbot_display, use_kb_checkbox, pdf_path_input],
        outputs=[chatbot_display]
    ).then(
        fn=lambda: "", # ì…ë ¥ì°½ ë¹„ìš°ê¸°
        inputs=None,
        outputs=user_input_textbox
    )

    # ì±„íŒ… í´ë¦¬ì–´ ë²„íŠ¼ í´ë¦­ ì‹œ
    clear_btn.click(
        fn=clear_chat,
        inputs=None,
        outputs=[user_input_textbox, chatbot_display]
    )

# --- ì•± ì‹¤í–‰ ---
if __name__ == "__main__":
    print("Gradio ì•±ì„ ì‹¤í–‰í•©ë‹ˆë‹¤. http://127.0.0.1:7860 ì—ì„œ í™•ì¸í•˜ì„¸ìš”.")
    demo.launch()
